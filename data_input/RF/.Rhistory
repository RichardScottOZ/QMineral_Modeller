#     dfnum[j,i] <- (dfnum[j,i] - min(dfnum[,i], na.rm = T))/(max(dfnum[,i], na.rm = T) - min(dfnum[,i], na.rm = T))
#   }
# }
dfnum <- sapply(dfnum, normalize)
df <- as_tibble(cbind(dflabs, dfnum))
df %>%
select(all_of(index))
} else {
index <- names(df)
k <- df %>%
select(all_of(Keep))
df <- df %>%
select(-all_of(Keep))
dflabs <- df[,!sapply(df, is.numeric)]
dfnum <- df[,sapply(df, is.numeric)]
# for (i in 1:ncol(dfnum)) {
#   for (j in 1:nrow(dfnum)) {
#     dfnum[j,i] <- (dfnum[j,i] - min(dfnum[,i], na.rm = T))/(max(dfnum[,i], na.rm = T) - min(dfnum[,i], na.rm = T))
#   }
# }
dfnum <- sapply(dfnum, normalize)
df <- as_tibble(cbind(dflabs, k, dfnum))
df %>%
select(all_of(index))
}
}
t <- rnorm(10)
t1 <- rnorm(100)
t2 <- rnorm(1000)
minmax.norm1(t)
minmax.norm1(tibble(t))
minmax.norm2(tibble(t))
minmax.norm2(tibble(t)) - minmax.norm1(tibble(t))
options(digits = 5)
minmax.norm2(tibble(t)) - minmax.norm1(tibble(t))
minmax.norm1(tibble(t))
minmax.norm2(tibble(t))
minmax.norm2(tibble(t)) - minmax.norm1(tibble(t))
t
mean(minmax.norm2(tibble(t1)) - minmax.norm1(tibble(t2)))
mean(minmax.norm2(tibble(t2)) - minmax.norm1(tibble(t2)))
mean(minmax.norm2(tibble(t2)) - minmax.norm1(tibble(t1)))
mean(minmax.norm2(tibble(t1)) - minmax.norm1(tibble(t1)))
mean(minmax.norm2(tibble(t1)) == minmax.norm1(tibble(t1)))
mean(minmax.norm2(tibble(t2)) == minmax.norm1(tibble(t2)))
mean(minmax.norm2(tibble(t3)) == minmax.norm1(tibble(t3)))
mean(minmax.norm2(tibble(t)) == minmax.norm1(tibble(t)))
t3 <- rnorm(10000)
mean(minmax.norm2(tibble(t3)) == minmax.norm1(tibble(t3)))
mean(minmax.norm2(tibble(t3)) == minmax.norm1(tibble(t3)))
install.packages('scheduler')
install.packages('taskscheduleR')
taskscheduleR:::taskschedulerAddin()
taskscheduleR:::taskschedulerAddin()
version
install.packages('benchmarkme')
library(benchmarkme)
res <- benchmark_std(runs = 3)
plot(res)
benchmarkme::get_cpu()
getwd()
library(parallel)
detectCores()
detectCores(logical = F)
install.packages('moderndive')
library(geoquimica)
?geoquimica
??geoquimica
library(stats)
rbinom(1000,10,0.1)
m=1000
n=10
p=0.1
a=rbinom(m,n,p)
table(a)
temp=c(a,seq(0,n))
temp2=table(temp)-1
temp2
rbinom(1000,10,0.5)
m=1000
n=10
p=0.5
a=rbinom(m,n,p)
table(a)
temp=c(a,seq(0,n))
temp3=table(temp)-1
temp3
rbinom(1000,10,0.9)
m=1000
n=10
p=0.9
a=rbinom(m,n,p)
table(a)
temp=c(a,seq(0,n))
temp4=table(temp)-1
temp4
rbind (temp2, temp3, temp4)
u <- rbind (temp2, temp3, temp4)
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade")
legend("topleft",
legend = levels(u),
fill = cols,
bty = "n")
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade")
legend("topleft",
legend = levels(u),
fill = cols,
bty = "n")
box(bty = "L")
class(u)
names(u)
u <- rbind(temp2, temp3, temp4)
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade")
legend("topleft",
legend = levels(u),
fill = cols,
bty = "n")
head(u)
rbind (temp2, temp3, temp4)
as.data.frame(rbind(temp2, temp3, temp4))
u$name <- rownames(u)
View(u)
u$name <- rownames(as.data.frame(u))
View(u)
u <- rbind(temp2, temp3, temp4)
u$name <- rownames(as.data.frame(u))
View(u)
library(data.table)
u <- rbind(temp2, temp3, temp4)
library(data.table)
u <- setDT(u,keep.rownames = TRUE)
u <- setDT(data.frame(u),keep.rownames = TRUE)
View(u)
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade")
barplot(u[2:12],
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade")
View(u)
library(stats)
rbinom(1000,10,0.1)
m=1000
n=10
p=0.1
a=rbinom(m,n,p)
table(a)
temp=c(a,seq(0,n))
temp2=table(temp)-1
temp2
rbinom(1000,10,0.5)
m=1000
n=10
p=0.5
a=rbinom(m,n,p)
table(a)
temp=c(a,seq(0,n))
temp3=table(temp)-1
temp3
rbinom(1000,10,0.9)
m=1000
n=10
p=0.9
a=rbinom(m,n,p)
table(a)
temp=c(a,seq(0,n))
temp4=table(temp)-1
temp4
rbind (temp2, temp3, temp4)
u <- rbind (temp2, temp3, temp4)
rbind (temp2, temp3, temp4,deparse.level = 1)
rbind (temp2, temp3, temp4,deparse.level = 2)
rbind (temp2, temp3, temp4,deparse.level = 1)
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade")
legend("topleft",
legend = levels(row.names(u)),
fill = cols,
bty = "n")
legend("topleft",
legend = levels(u),
fill = cols,
bty = "n")
box(bty = "L")
set.seed(1)
rbinom(1000,10,0.1)
m=1000
n=10
p=0.1
a=rbinom(m,n,p)
table(a)
temp=c(a,seq(0,n))
temp2=table(temp)-1
temp2
rbinom(1000,10,0.5)
m=1000
n=10
p=0.5
a=rbinom(m,n,p)
table(a)
temp=c(a,seq(0,n))
temp3=table(temp)-1
temp3
rbinom(1000,10,0.9)
m=1000
n=10
p=0.9
a=rbinom(m,n,p)
table(a)
temp=c(a,seq(0,n))
temp4=table(temp)-1
temp4
rbind (temp2, temp3, temp4,deparse.level = 1)
u <- rbind (temp2, temp3, temp4)
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade")
legend("topleft",
legend = levels(u),
fill = cols,
bty = "n")
box(bty = "L")
levels(u)
row.names(u)
legend("topleft",
legend = row.names(u),
fill = cols,
bty = "n")
legend("topleft",
legend = row.names(u),
fill = row.names(u),
bty = "n")
legend("topleft",
legend = row.names(u),
fill = c('red','blue','green'),
bty = "n")
box(bty = "L")
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade",col = c('red','blue','green'))
legend("topleft",
legend = row.names(u),
fill = c('red','blue','green'),
bty = "n")
box(bty = "L")
legend("bottom",
legend = row.names(u),
fill = c('red','blue','green'),
bty = "n")
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade",col = c('red','blue','green'))
legend("bottom",
legend = row.names(u),
fill = c('red','blue','green'),
bty = "n")
box(bty = "L")
legend("lower",
legend = row.names(u),
fill = c('red','blue','green'),
bty = "n")
legend("topleft",
legend = row.names(u),
fill = c('red','blue','green'),
bty = "n")
legend("topright",
legend = row.names(u),
fill = c('red','blue','green'),
bty = "n")
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade",col = c('red','blue','green'))
legend("topright",
legend = row.names(u),
fill = c('red','blue','green'),
bty = "n")
box(bty = "L")
u <- rbind(temp2, temp3, temp4,deparse.level = 1)
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade",col = c('red','blue','green'))
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade",col = c('red','blue','green'))
u <- rbind(temp2, temp3, temp4,deparse.level = 1)
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade",col = c('red','blue','green'))
legend("topright",
legend = row.names(u),
fill = c('red','blue','green'),
bty = "n")
box(bty = "L")
box(bty = "L",lty = 'L')
box(bty = "L")
barplot(u,
beside = TRUE,
xlab = "Valores",
ylab = "função de probabilidade",col = c('red','blue','green'))
legend("topright",
legend = row.names(u),
fill = c('red','blue','green'),
bty = "n")
box(bty = "L")
library(randomForest)
library(ggplot2)
library(data.table)
set.seed(59761) # chosen by rollig 5d10
`%nin%` <- function(x,y)!(`%in%`(x,y))
SMOOTHER <- 0.5 # "prior" on classification decisions
NTREE <- 1024 # number of trees to grow
dat <- data.table(diamonds)
get_binary_labels <- function(labels){
out <- ifelse(labels == "High", 1, 0)
return(out)
}
dat[,price_bin := as.factor(ifelse(price > median(price), "High", "Low"))]
dat[,price := NULL]
all_row_ndx <- 1:nrow(dat)
test_ndx <- sample(1:nrow(dat), size=nrow(dat) / 15)
train_ndx <- all_row_ndx[all_row_ndx %nin% test_ndx]
# This takes ~25s on my laptop
system.time(
{
rf <- randomForest(price_bin ~ .,
data=dat[train_ndx,],
ntree=NTREE,
mtry=3,
nodesize=5,
maxnodes=512)
}
)
y_pred <- predict(rf, newdata=dat[test_ndx,], predict.all=TRUE)
y_pred_tree <- get_binary_labels(y_pred$individual)
y_pred_cumulative <- t(apply(y_pred_tree, 1, cumsum))
rescale_mat <- diag(1 / (1:NTREE + 2 * SMOOTHER))
y_pred_smoothed_probs <- (SMOOTHER + y_pred_cumulative) %*% rescale_mat
binary_cross_entropy <- function(y_pred, y_true, epsilon=1e-7){
# only need to clip the scores if you're not using smoothing
# y_pred_tilde <- sapply(y_pred, function(x) median(c(1 - epsilon, x, epsilon)))
scores <- y_true * y_pred + (1 - y_true) * (1 - y_pred)
loss <- -mean(log(scores))
return(loss)
}
y_test_binary <- get_binary_labels(dat[test_ndx,price_bin])
xe <- apply(y_pred_smoothed_probs, 2, binary_cross_entropy, y_true=y_test_binary)
plot(xe, type="l", xlab="ntree", ylim=c(0.04, 0.075), main="cross entropy vs ntree")
abline(h=-log(0.5), col="red", lty="dashed")
accuracy <- function(y_pred, y_true){
scores <- (y_pred > 0.5) == y_true
acc <- mean(scores)
return(acc)
}
acc <- apply(y_pred_smoothed_probs, 2, accuracy, y_true=y_test_binary)
scores_dt <- data.table(cross_entropy=xe, accuracy=acc, ntree=1:NTREE)
# train a second tree using this seed
set.seed(16795)
rf2 <- randomForest(price_bin ~ .,
data=dat[train_ndx,],
ntree=NTREE,
mtry=3,
nodesize=5,
maxnodes=512)
y_pred2 <- predict(rf2, newdata=dat[test_ndx,], predict.all=TRUE)
y_pred_tree2 <- get_binary_labels(y_pred2$individual)
y_pred_cumulative2 <- t(apply(y_pred_tree2, 1, cumsum))
rescale_mat2 <- diag(1 / (1:NTREE + 2 * SMOOTHER))
y_pred_smoothed_probs2 <- (SMOOTHER + y_pred_cumulative2) %*% rescale_mat2
acc2 <- apply(y_pred_smoothed_probs2, 2, accuracy, y_true=y_test_binary)
xe2 <- apply(y_pred_smoothed_probs2, 2, binary_cross_entropy, y_true=y_test_binary)
rf_compare <- data.table(ntree=c(1:NTREE, 1:NTREE),
accuracy=c(acc, acc2),
rf_seed=c(rep("rf1", NTREE), rep("rf2", NTREE)),
cross_entropy=c(xe, xe2))
ggplot(rf_compare, aes(x=ntree, y=accuracy, color=rf_seed)) + geom_line() + ggtitle("Accuracy vs trees is just noise")
ggsave("rf_accuracy.png")
ggplot(rf_compare, aes(x=ntree, y=cross_entropy, color=rf_seed)) + geom_line() + ggtitle("Cross-entropy vs ntree is not volatile")
ggsave("rf_cross_entropy.png")
library(randomForest)
library(ggplot2)
library(data.table)
# set.seed(59761) # chosen by rollig 5d10
set.seed(0)
`%nin%` <- function(x,y)!(`%in%`(x,y))
SMOOTHER <- 0.5 # "prior" on classification decisions
NTREE <- 1024 # number of trees to grow
dat <- data.table(diamonds)
get_binary_labels <- function(labels){
out <- ifelse(labels == "High", 1, 0)
return(out)
}
dat[,price_bin := as.factor(ifelse(price > median(price), "High", "Low"))]
dat[,price := NULL]
all_row_ndx <- 1:nrow(dat)
test_ndx <- sample(1:nrow(dat), size=nrow(dat) / 15)
train_ndx <- all_row_ndx[all_row_ndx %nin% test_ndx]
# This takes ~25s on my laptop
system.time(
{
rf <- randomForest(price_bin ~ .,
data=dat[train_ndx,],
ntree=NTREE,
mtry=3,
nodesize=5,
maxnodes=512)
}
)
y_pred <- predict(rf, newdata=dat[test_ndx,], predict.all=TRUE)
y_pred_tree <- get_binary_labels(y_pred$individual)
y_pred_cumulative <- t(apply(y_pred_tree, 1, cumsum))
rescale_mat <- diag(1 / (1:NTREE + 2 * SMOOTHER))
y_pred_smoothed_probs <- (SMOOTHER + y_pred_cumulative) %*% rescale_mat
binary_cross_entropy <- function(y_pred, y_true, epsilon=1e-7){
# only need to clip the scores if you're not using smoothing
# y_pred_tilde <- sapply(y_pred, function(x) median(c(1 - epsilon, x, epsilon)))
scores <- y_true * y_pred + (1 - y_true) * (1 - y_pred)
loss <- -mean(log(scores))
return(loss)
}
y_test_binary <- get_binary_labels(dat[test_ndx,price_bin])
xe <- apply(y_pred_smoothed_probs, 2, binary_cross_entropy, y_true=y_test_binary)
plot(xe, type="l", xlab="ntree", ylim=c(0.04, 0.075), main="cross entropy vs ntree")
abline(h=-log(0.5), col="red", lty="dashed")
accuracy <- function(y_pred, y_true){
scores <- (y_pred > 0.5) == y_true
acc <- mean(scores)
return(acc)
}
acc <- apply(y_pred_smoothed_probs, 2, accuracy, y_true=y_test_binary)
scores_dt <- data.table(cross_entropy=xe, accuracy=acc, ntree=1:NTREE)
# train a second tree using this seed
# set.seed(16795)
set.seed(0)
rf2 <- randomForest(price_bin ~ .,
data=dat[train_ndx,],
ntree=NTREE,
mtry=3,
nodesize=5,
maxnodes=512)
y_pred2 <- predict(rf2, newdata=dat[test_ndx,], predict.all=TRUE)
y_pred_tree2 <- get_binary_labels(y_pred2$individual)
y_pred_cumulative2 <- t(apply(y_pred_tree2, 1, cumsum))
rescale_mat2 <- diag(1 / (1:NTREE + 2 * SMOOTHER))
y_pred_smoothed_probs2 <- (SMOOTHER + y_pred_cumulative2) %*% rescale_mat2
acc2 <- apply(y_pred_smoothed_probs2, 2, accuracy, y_true=y_test_binary)
xe2 <- apply(y_pred_smoothed_probs2, 2, binary_cross_entropy, y_true=y_test_binary)
rf_compare <- data.table(ntree=c(1:NTREE, 1:NTREE),
accuracy=c(acc, acc2),
rf_seed=c(rep("rf1", NTREE), rep("rf2", NTREE)),
cross_entropy=c(xe, xe2))
ggplot(rf_compare, aes(x=ntree, y=accuracy, color=rf_seed)) + geom_line() + ggtitle("Accuracy vs trees is just noise")
ggsave("rf_accuracy.png")
ggplot(rf_compare, aes(x=ntree, y=cross_entropy, color=rf_seed)) + geom_line() + ggtitle("Cross-entropy vs ntree is not volatile")
ggsave("rf_cross_entropy.png")
setwd('C:/Users/GUILHERMEFERREIRA-PC/Documents/GitHub/MinChem_Modeller/data_input/RF')
felds <- read.csv('feldspar_rf.csv')
glimpse(felds)
library(tidyverse)
glimpse(felds)
names(felds)
sort(unique(felds$MINERAL))
felds %>%
select(19, 25:46) %>%
filter(MINERAL != '(AL)KALIFELDSPAR',
MINERAL != 'FELDSPAR',)
df <- felds %>%
select(19, 25:46) %>%
filter(MINERAL != '(AL)KALIFELDSPAR',
MINERAL != 'FELDSPAR',)
df <- felds %>%
select(19, 25:46) %>%
filter(MINERAL != '(AL)KALIFELDSPAR',
MINERAL != 'FELDSPAR',) %>%
group(MINERAL) %>%
sample_n(50,replace = TRUE)
df <- felds %>%
select(19, 25:46) %>%
filter(MINERAL != '(AL)KALIFELDSPAR',
MINERAL != 'FELDSPAR',) %>%
group_by(MINERAL) %>%
sample_n(50,replace = TRUE)
View(df)
write.csv(df, 'C:/Users/GUILHERMEFERREIRA-PC/Desktop/Curso Congresso Brasileiro de Geologia/quimica_mineral.csv')
df <- felds %>%
select(19, 25:46) %>%
filter(MINERAL != '(AL)KALIFELDSPAR',
MINERAL != 'FELDSPAR',) %>%
group_by(MINERAL) %>%
sample_n(50,replace = TRUE) %>%
ungroup() %>%
select(-MINERAL)
write.csv(df, 'C:/Users/GUILHERMEFERREIRA-PC/Desktop/Curso Congresso Brasileiro de Geologia/quimica_mineral.csv')
